 /**********************
**********************/

#include <iostream>
#include <string>
#include <opencv2/opencv.hpp>
#include <ros/ros.h>
#include <std_msgs/Float32MultiArray.h>
#include <cv_bridge/cv_bridge.h>
#include <geometry_msgs/PointStamped.h>
#include <image_transport/image_transport.h>
#include <sensor_msgs/CameraInfo.h>
#include <sensor_msgs/Image.h>
#include <sensor_msgs/PointCloud2.h>
#include <pcl/point_cloud.h>
#include <pcl/point_types.h>
#include <pcl/conversions.h>
#include <pcl/pcl_macros.h>
#include <pcl_conversions/pcl_conversions.h>

using namespace cv;
using namespace std;
class ImageConverter {
private:
	ros::NodeHandle			nh_;
	image_transport::ImageTransport it_;
	image_transport::Subscriber	image_sub_color;//接收彩色图像
	image_transport::Subscriber	image_sub_depth;//接收深度图像

	ros::Subscriber camera_info_sub_;//接收深度图像对应的相机参数话题
	ros::Subscriber semantic_info_sub_;//接收深度图像对应的相机参数话题
	ros::Publisher	arm_point_pub_;//发布一个三维坐标点，可用于可视化
	ros::Publisher pcl_pub; //发布语义点云

	sensor_msgs::CameraInfo		camera_info;
	
	geometry_msgs::PointStamped	output_point;
	int  imageWidth=1280;
	int imageHeight=720;
	/* Mat depthImage,colorImage; */
	Mat	colorImage;
	Mat	depthImage	= Mat::zeros( imageWidth, imageHeight, CV_16UC1 );//注意这里要修改为你接收的深度图像尺寸
	Mat	semantic_info= Mat::zeros( imageWidth, imageHeight, CV_8UC1 );
	Point	mousepos	= Point( 0, 0 );        /* mousepoint to be map */

public:
	//获取鼠标的坐标，通过param指针传出到类成员Point mousepos
	static void on_mouse( int event, int x, int y, int flags, void *param )
	{
		switch ( event )
		{
		case CV_EVENT_MOUSEMOVE:                /* move mouse */
		{
			Point &tmppoint = *(cv::Point *) param;
			tmppoint = Point( x, y );
		} break;
		}
	}


	ImageConverter() : it_( nh_ )
	{
    //topic sub:
		image_sub_depth = it_.subscribe( "/d435i/aligned_depth_to_color/image_raw",
						 1, &ImageConverter::imageDepthCb, this );
		image_sub_color = it_.subscribe( "/d435i/color/image_raw", 1,
						 &ImageConverter::imageColorCb, this );
		camera_info_sub_ =
			nh_.subscribe( "/d435i/aligned_depth_to_color/camera_info", 1,
				       &ImageConverter::cameraInfoCb, this );
		 semantic_info_sub_=
		 	nh_.subscribe("/semantic_info",1,&ImageConverter::semanticInfoCb,this);
    
    //topic pub:
		pcl_pub = nh_.advertise<sensor_msgs::PointCloud2> ("semantic_pcl", 10);
		cv::namedWindow( "colorImage" );
		setMouseCallback( "colorImage", &ImageConverter::on_mouse,
				  (void *) &mousepos );
	}


	~ImageConverter()
	{
		cv::destroyWindow( "colorImage" );
	}


	void cameraInfoCb( const sensor_msgs::CameraInfo &msg )
	{
		camera_info = msg;
	}

	void semanticInfoCb( const sensor_msgs::ImageConstPtr &msg )
	{
		cv_bridge::CvImagePtr cv_ptr;
		try {
			cv_ptr =
				cv_bridge::toCvCopy( msg, sensor_msgs::image_encodings::TYPE_8UC1 );
			semantic_info = cv_ptr->image;
		} catch ( cv_bridge::Exception &e ) {
			ROS_ERROR( "cv_bridge exception: %s", e.what() );
			return;
		}
		semanticUpdate();
	}


	void imageDepthCb( const sensor_msgs::ImageConstPtr &msg )
	{
		cv_bridge::CvImagePtr cv_ptr;

		try {
			cv_ptr =
				cv_bridge::toCvCopy( msg, sensor_msgs::image_encodings::TYPE_16UC1 );
			depthImage = cv_ptr->image;
		} catch ( cv_bridge::Exception &e ) {
			ROS_ERROR( "cv_bridge exception: %s", e.what() );
			return;
		}
	}


	void imageColorCb( const sensor_msgs::ImageConstPtr &msg )
	{
		cv_bridge::CvImagePtr cv_ptr;
		try {
			cv_ptr		= cv_bridge::toCvCopy( msg, sensor_msgs::image_encodings::BGR8 );
			colorImage	= cv_ptr->image;
		} catch ( cv_bridge::Exception &e ) {
			ROS_ERROR( "cv_bridge exception: %s", e.what() );
			return;
		}
		
		// //先查询对齐的深度图像的深度信息，根据读取的camera info内参矩阵求解对应三维坐标
		// float	tmp_z	= 0.001 * depthImage.at<u_int16_t>( mousepos.y, mousepos.x );
		// float	tmp_x	=
		// 	(mousepos.x - camera_info.K.at( 2 ) ) / camera_info.K.at( 0 ) * tmp_z;
		// float tmp_y =
		// 	(mousepos.y - camera_info.K.at( 5 ) ) / camera_info.K.at( 4 ) * tmp_z;
		
		// char tam[100];
		// float real_x=tmp_z;
		// float real_y=-tmp_x;
		// float real_z=-tmp_y+0.38;
		// sprintf( tam, "(%0.2f,%0.2f,%0.2f)", real_x,real_y, real_z );
		// putText( colorImage, tam, mousepos, FONT_HERSHEY_SIMPLEX, 0.6,
		// 	 cvScalar( 0, 0, 255 ), 1 );//打印到屏幕上
		// circle( colorImage, mousepos, 2, Scalar( 255, 0, 0 ) );
		
		// cv::imshow( "colorImage", colorImage );
		// cv::waitKey( 1 );
	}
	void semanticUpdate()//update semantic info
	{
		pcl::PointCloud<pcl::PointXYZRGB>::Ptr cloud(new pcl::PointCloud<pcl::PointXYZRGB>);
		sensor_msgs::PointCloud2 output;
		
		//Mat semantic_pic=imread("/home/qingshan/catkin_ws/src/Pytorch-UNet-master/info.jpg");

    	//把点云转化为ros消息
		for(int u=0;u<imageWidth;u++)//该循环遍历深度图的每一个像素
			for(int v=0;v<imageHeight;v++)
			{
				float	tmp_z	= 0.001 * depthImage.at<u_int16_t>( v, u );//获取该点深度值并转换单位为米，即z值
				float	tmp_x	=(u - camera_info.K.at( 2 ) ) / camera_info.K.at( 0 ) * tmp_z;
				float tmp_y =(v- camera_info.K.at( 5 ) ) / camera_info.K.at( 4 ) * tmp_z;//根据相机内参计算该像素点对应的xy坐标
				//输入参数为像素点在图像上的坐标，像素点对应深度值，输出为像素点的三维坐标，该坐标相对于深度参考系


				float real_x=tmp_z;
				float real_y=-tmp_x;
				float real_z=-tmp_y+0.38;
				//RGB参考系相对于深度系有一个固定变换，手动进行，加的定值是相机离地高度
				pcl::PointXYZRGB p;
				p.x=real_x;p.y=real_y;//p.z=1;
				p.z=real_z;
				//将RGB中的坐标传入点云xyz信息，下面是把语义信息存入点云的b通道
				int info=semantic_info.at<u_int16_t>(v,u);
				p.r=0;p.g=0;
				switch (info)
				{
					case 0:
						p.b=0;
					break;
					case 1:
						p.b=1;
					break;
					case 2:
						p.b=8;
					break;
					case 3:
						p.b=4;
					break;
					case 4:
						p.b=1;
					break;
					case 5:
						p.b=2;
					break;
					case 6:
						p.b=3;
					break;
				
					default:
						p.b=0;
					break;
				}
				cloud->points.push_back(p);
			}
		const pcl::PointCloud<pcl::PointXYZRGB>::Ptr cloudx=cloud;
		pcl::toROSMsg(*cloudx, output);
		//pcl_conversions::fromPCL(*cloudx, output);
    	output.header.frame_id = "/d435i_color_frame";
		output.header.stamp=ros::Time::now();
		pcl_pub.publish(output);
	}
};

int main( int argc, char **argv )
{
	ros::init( argc, argv, "coordinate_map" );
	ImageConverter imageconverter;
	ros::spin();
	return(0);
}


